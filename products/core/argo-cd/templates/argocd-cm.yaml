apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-cm
  namespace: argocd
  labels:
    app.kubernetes.io/name: argocd-cm
    app.kubernetes.io/part-of: argocd
data:
  url: "https://argocd.feddema.dev"

  users.anonymous.enabled: "false"

  accounts.infro: "apiKey"
  accounts.kubechecks: "apiKey"

  admin.enabled: "false"

  timeout.reconciliation: "180s"
  cluster.inClusterEnabled: "true"

  resource.customizations.ignoreDifferences.admissionregistration.k8s.io_ValidatingWebhookConfiguration: |
    jqPathExpressions:
    - '.webhooks[]?.clientConfig.caBundle'
  resource.customizations.ignoreDifferences.admissionregistration.k8s.io_MutatingWebhookConfiguration: |
    jqPathExpressions:
    - '.webhooks[]?.clientConfig.caBundle'

  resource.customizations.ignoreResourceUpdates.all: |
    jsonPointers:
      - /status

  resource.customizations.ignoreResourceUpdates.argoproj.io_Application: |
    jqPathExpressions:
      - '.metadata.annotations."notified.notifications.argoproj.io"'
      - '.metadata.annotations."argocd.argoproj.io/refresh"'
      - '.metadata.annotations."argocd.argoproj.io/hydrate"'
      - '.operation'

  resource.customizations.ignoreResourceUpdates.argoproj.io_Rollout: |
    jqPathExpressions:
      - '.metadata.annotations."notified.notifications.argoproj.io"'

  resource.customizations.ignoreResourceUpdates.autoscaling_HorizontalPodAutoscaler: |
    jqPathExpressions:
      - '.metadata.annotations."autoscaling.alpha.kubernetes.io/behavior"'
      - '.metadata.annotations."autoscaling.alpha.kubernetes.io/conditions"'
      - '.metadata.annotations."autoscaling.alpha.kubernetes.io/metrics"'
      - '.metadata.annotations."autoscaling.alpha.kubernetes.io/current-metrics"'

  resource.customizations.ignoreResourceUpdates.ConfigMap: |
    jqPathExpressions:
      # Ignore the cluster-autoscaler status
      - '.metadata.annotations."cluster-autoscaler.kubernetes.io/last-updated"'
      # Ignore the annotation of the legacy Leases election
      - '.metadata.annotations."control-plane.alpha.kubernetes.io/leader"'

  resource.customizations.ignoreResourceUpdates.apps_ReplicaSet: |
    jqPathExpressions:
      - '.metadata.annotations."deployment.kubernetes.io/desired-replicas"'
      - '.metadata.annotations."deployment.kubernetes.io/max-replicas"'
      - '.metadata.annotations."rollout.argoproj.io/desired-replicas"'

  resource.customizations.ignoreResourceUpdates.discovery.k8s.io_EndpointSlice: |
    jsonPointers:
      - /metadata
      - /endpoints
      - /ports

  resource.customizations.ignoreResourceUpdates.Endpoints: |
    jsonPointers:
      - /metadata
      - /subsets

  resource.exclusions: |
    ### Network resources created by the Kubernetes control plane and excluded to reduce the number of watched events and UI clutter
    - apiGroups:
      - ''
      - discovery.k8s.io
      kinds:
      - Endpoints
    ### Internal Kubernetes resources excluded reduce the number of watched events
    - apiGroups:
      - coordination.k8s.io
      kinds:
      - Lease
    ### Internal Kubernetes Authz/Authn resources excluded reduce the number of watched events
    - apiGroups:
      - authentication.k8s.io
      - authorization.k8s.io
      kinds:
      - SelfSubjectReview
      - TokenReview
      - LocalSubjectAccessReview
      - SelfSubjectAccessReview
      - SelfSubjectRulesReview
      - SubjectAccessReview
    ### Intermediate Certificate Request excluded reduce the number of watched events
    - apiGroups:
      - certificates.k8s.io
      kinds:
      - CertificateSigningRequest
    - apiGroups:
      - cert-manager.io
      kinds:
      - CertificateRequest
    ### Cilium internal resources excluded reduce the number of watched events and UI Clutter
    - apiGroups:
      - cilium.io
      kinds:
      - CiliumIdentity
      - CiliumEndpoint
      - CiliumEndpointSlice
    ### Kyverno intermediate and reporting resources excluded reduce the number of watched events and improve performance
    - apiGroups:
      - kyverno.io
      - reports.kyverno.io
      - wgpolicyk8s.io
      - openreports.io
      kinds:
      - Report
      - ClusterReport
      - PolicyReport
      - ClusterPolicyReport
      - EphemeralReport
      - ClusterEphemeralReport
      - AdmissionReport
      - ClusterAdmissionReport
      - BackgroundScanReport
      - ClusterBackgroundScanReport
      - UpdateRequest
    ### Trivy Operator intermediate and reporting resources excluded reduce the number of watched events and improve performance
    - apiGroups:
      - aquasecurity.github.io
      kinds:
      - SbomReport
      - ClusterSbomReport
      - VulnerabilityReport
      - ClusterVulnerabilityReport
      - ConfigAuditReport
      - ClusterConfigAuditReport
      - RbacAssessmentReport
      - ClusterRbacAssessmentReport
      - ComplianceReport
      - InfraAssessmentReport
      - ClusterInfraAssessmentReport
      - ExposedSecretReport

  resource.customizations.ignoreDifferences.argoproj.io_Application: |
    jqPathExpressions:
    - .metadata.finalizers[]? | select(. == "post-delete-finalizer.argocd.argoproj.io" or . == "post-delete-finalizer.argocd.argoproj.io/cleanup")
    - if (.metadata.finalizers | length) == 0 then .metadata.finalizers else empty end
  
  resource.customizations.health.ceph.rook.io_CephCluster: |
    hs = {}

    if obj.status ~= nil and obj.status.ceph ~= nil then
      local health = obj.status.ceph.health
      if health == "HEALTH_OK" then
        hs.status = "Healthy"
        hs.message = "Ceph health is OK"
      elseif health == "HEALTH_WARN" then
        hs.status = "Healthy"
        hs.message = "Ceph cluster has warnings"
      elseif health == "HEALTH_ERR" then
        hs.status = "Degraded"
        hs.message = "Ceph cluster is in error state"
      else
        hs.status = "Unknown"
        hs.message = "Ceph cluster health: " .. tostring(health)
      end
    else
      hs.status = "Progressing"
      hs.message = "CephCluster status not available"
    end

    return hs

  resource.customizations.health.csiaddons.openshift.io_ReclaimSpaceJob: |
    hs = {}
    status = obj.status or {}

    if status.result == "Succeeded" then
      hs.status = "Healthy"
      hs.message = "ReclaimSpaceJob completed successfully."
    elseif status.result == "Failed" then
      hs.status = "Degraded"
      hs.message = "ReclaimSpaceJob is failed."
    else
      hs.status = "Progressing"
      hs.message = "ReclaimSpaceJob is progressing."
    end
    return hs

  resource.customizations.health.velero.io_DataUpload: |
    hs = {}
    status = obj.status or {}
    phase  = status.phase or ""
    message = status.message or ""
    if phase == "Completed" then
      hs.status = "Healthy"
      hs.message = "DataUpload completed successfully. " .. message
    elseif phase == "Failed" then
      hs.status = "Degraded"
      hs.message = "DataUpload failed. " .. message
    elseif phase == "Cancelled" or phase == "Canceled" then
      hs.status = "Degraded"
      hs.message = "DataUpload was cancelled. " .. message
    elseif phase == "InProgress" then
      hs.status = "Progressing"
      hs.message = "DataUpload in progress. " .. message
    elseif phase == "Accepted" or phase == "Pending" then
      hs.status = "Progressing"
      hs.message = "DataUpload accepted / pending. " .. message
    else
      hs.status = "Unknown"
      hs.message = "DataUpload phase unknown (" .. phase .. "). " .. message
    end
    return hs

  resource.customizations.health.velero.io_Backup: |
    hs = {}
    status = obj.status or {}
    phase  = status.phase or ""
    message = status.message or ""
    if phase == "Completed" then
      hs.status = "Healthy"
      hs.message = "Backup succeeded. " .. (message or "")
    elseif phase == "PartiallyFailed" then
      hs.status = "Degraded"
      hs.message = "Backup completed with partial failures. " .. (message or "")
    elseif phase == "Failed" or phase == "FailedValidation" then
      hs.status = "Degraded"
      hs.message = "Backup failed (" .. phase .. "). " .. (message or "")
    elseif phase == "Deleting" then
      hs.status = "Progressing"
      hs.message = "Backup is being deleted."
    elseif phase == "InProgress" then
      hs.status = "Progressing"
      hs.message = "Backup in progress. " .. (message or "")
    elseif phase == "New" then
      hs.status = "Progressing"
      hs.message = "Backup created, awaiting start."
    else
      hs.status = "Unknown"
      hs.message = "Backup phase unknown (" .. phase .. "). " .. (message or "")
    end
    return hs

  resource.customizations.actions.postgresql.cnpg.io_Cluster: |
    discovery.lua: |
      actions = {}
      actions["restart"] = {}
      actions["reload"] = {}
      actions["failover"] = {}
      return actions

    definitions:
      - name: restart
        action.lua: |
          local os = require("os")
          if obj.metadata == nil then
            obj.metadata = {}
          end
          if obj.metadata.annotations == nil then
            obj.metadata.annotations = {}
          end
          obj.metadata.annotations["kubectl.kubernetes.io/restartedAt"] = os.date("!%Y-%m-%dT%XZ")
          return obj
      - name: reload
        action.lua: |
          local os = require("os")
          if obj.metadata == nil then
            obj.metadata = {}
          end
          if obj.metadata.annotations == nil then
            obj.metadata.annotations = {}
          end
          obj.metadata.annotations["cnpg.io/reloadedAt"] = os.date("!%Y-%m-%dT%XZ")
          return obj
      - name: failover
        action.lua: |
          local os = require("os")
          local nextIndex = 0
          for index, node in pairs(obj.status.instancesStatus.healthy) do
            if node == obj.status.currentPrimary then
              nextIndex = index + 1
              if nextIndex > #obj.status.instancesStatus.healthy then
                nextIndex = 1
              end
              break
            end
          end
          if nextIndex > 0 then
            obj.status.targetPrimary = obj.status.instancesStatus.healthy[nextIndex]
            obj.status.targetPrimaryTimestamp = os.date("!%Y-%m-%dT%XZ")
          end
          return obj

  server.rbac.log.enforce.enable: "false"

  exec.enabled: "true"
  exec.shells: "bash,sh,powershell,cmd"

  oidc.tls.insecure.skip.verify: "false"
  oidc.config: |
    name: Authentik
    issuer: https://authentik.feddema.dev/application/o/argocd/
    clientID: 7uRmOU7Cho0KwDa7SIODe0ugmQ5djIvDuN3wUeux
    clientSecret: $authentik-credentials:clientSecret
    requestedScopes: ["openid", "profile", "email", "groups"]
    logoutURL: https://authentik.feddema.dev/application/o/argocd/end-session/

