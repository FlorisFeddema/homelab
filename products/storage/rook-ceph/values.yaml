gatus-monitor:
  appName: ceph
  publicEndpoints:
    - name: ceph
  privateEndpoints:
    - name: ceph

route:
  enabled: true
  hostnames:
    - ceph.feddema.dev
  parentRefs:
    - name: envoy-gateway-public
      namespace: envoy-gateway
      sectionName: web-https

rook-ceph:
  revisionHistoryLimit: "0"
  resources:
    requests:
      cpu: 1
      memory: 128Mi
    limits:
      memory: 1Gi

  monitoring:
    enabled: true

  csi:
    enableMetadata: true
    enableOMAPGenerator: true
    enableLiveness: true
    serviceMonitor:
      enabled: true
    cephFSKernelMountOptions: "ms_mode=secure"

    csiAddons:
      enabled: false

    topology:
      enabled: true
      domainLabels:
        - kubernetes.io/hostname
        - topology.kubernetes.io/zone

    csiRBDProvisionerResource: |
      - name : csi-provisioner
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : csi-omap-generator
        resource:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiRBDPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 30Mi
          limits:
            memory: 100Mi
      - name : csi-rbdplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSProvisionerResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-provisioner
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSPluginResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 20Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi

#######################################################
rook-ceph-cluster:
  toolbox:
    enabled: true

  monitoring:
    enabled: true
    createPrometheusRules: true

  cephClusterSpec:
    cephVersion:
      image: quay.io/ceph/ceph:v19.2.2

    network:
      connections:
        encryption:
          enabled: true
        compression:
          enabled: true
        requireMsgr2: true
      provider: host
      addressRanges:
        public: ['192.168.4.0/24']
        cluster: ['192.168.5.0/24']

    cephConfig:
      global:
        mon_warn_on_pool_no_redundancy: 'false'
        osd_pool_default_size: '2'
        osd_pool_default_min_size: '1'
      mgr:
        mgr/dashboard/standby_behaviour: 'error'
        mgr/dashboard/FEATURE_TOGGLE_NFS: 'false'
        mgr/dashboard/FEATURE_TOGGLE_ISCSI: 'false'
        mgr/dashboard/FEATURE_TOGGLE_MIRRORING: 'false'

    dashboard:
      ssl: false
      prometheusEndpoint: http://prometheus-prometheus.prometheus.svc:9090/

    csi:
      readAffinity:
        enabled: true

    placement:
      mon:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: NotIn
                    values:
                      - kashaylan
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
      mgr:
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule

    mon:
      count: 3

    mgr:
      count: 2
      modules:
        - name: pg_autoscaler
          enabled: true
        - name: insights
          enabled: true
        - name: rgw
          enabled: true

    storage:
      useAllNodes: false
      useAllDevices: false
      allowDeviceClassUpdate: true
      nodes:
        - name: hortek-0
          devices:
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030303937365031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030323137395031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030343131325031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
        - name: korris-1
          devices:
            - name: /dev/disk/by-id/nvme-eui.e8238fa6bf530001001b448b4aee9594
              deviceClass: nvme
        - name: tasuq-1
          devices:
            - name: /dev/disk/by-id/nvme-eui.0025385a91b29401
              deviceClass: nvme
#        - name: kashaylan-3
#          devices:
#            - name: /dev/disk/by-id/nvme-eui.e8238fa6bf530001001b448b4aeeb60d
#              deviceClass: nvme
    resources:
      mgr:
        requests:
          cpu: 500m
          memory: 512Mi
        limits:
          memory: 1Gi
      mon:
        requests:
          cpu: 1000m
          memory: 1Gi
        limits:
          memory: 2Gi
      osd:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          memory: 4Gi
      prepareosd:
        requests:
          cpu: 500m
          memory: 50Mi
      mgr-sidecar:
        requests:
          cpu: 100m
          memory: 40Mi
        limits:
          memory: 100Mi
      crashcollector:
        requests:
          cpu: 10m
          memory: 50Mi
        limits:
          memory: 100Mi
      logcollector:
        requests:
          cpu: 10m
          memory: 10Mi
        limits:
          memory: 100Mi
      cleanup:
        requests:
          cpu: 500m
          memory: 100Mi
        limits:
          memory: 1Gi
      exporter:
        requests:
          cpu: 50m
          memory: 50Mi
        limits:
          memory: 128Mi

  cephBlockPoolsVolumeSnapshotClass:
    enabled: true
    name: ceph-block
    isDefault: true
    deletionPolicy: Delete
    labels:
      velero.io/csi-volumesnapshot-class: "true"
  cephFileSystemVolumeSnapshotClass:
    enabled: true
    name: ceph-filesystem
    isDefault: false
    deletionPolicy: Delete
    labels:
      velero.io/csi-volumesnapshot-class: "true"

  cephBlockPools:
    - name: ceph-block-nvme-1
      spec:
        failureDomain: zone
        replicated:
          size: 1
        enableRBDStats: true
        deviceClass: nvme
        enableCrushUpdates: true
      storageClass:
        enabled: true
        name: ceph-block-nvme-1
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    - name: ceph-block-nvme-2
      spec:
        failureDomain: zone
        replicated:
          size: 2
        enableRBDStats: true
        deviceClass: nvme
        enableCrushUpdates: true
      storageClass:
        enabled: true
        name: ceph-block-nvme-2
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    - name: ceph-block-hdd-1
      spec:
        failureDomain: zone
        replicated:
          size: 1
        enableRBDStats: true
        deviceClass: hdd
        enableCrushUpdates: true
      storageClass:
        enabled: true
        name: ceph-block-hdd-1
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

  cephFileSystems:
    - name: ceph-file-nvme-2
      spec:
        metadataPool:
          enableCrushUpdates: true
          failureDomain: zone
          replicated:
            size: 2
        dataPools:
          - name: data0
            enableCrushUpdates: true
            failureDomain: zone
            replicated:
              size: 2
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            requests:
              cpu: 10m
              memory: 1Gi
            limits:
              memory: 3Gi
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-file-nvme-2
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: { }
        labels: { }
        mountOptions: [ ]
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

  cephObjectStores:
    - name: ceph-object-nvme-2
      spec:
        metadataPool:
          failureDomain: zone
          replicated:
            size: 2
          deviceClass: nvme
        dataPool:
          failureDomain: zone
          replicated:
            size: 2
          deviceClass: nvme
        allowUsersInNamespaces: ['*']
        gateway:
          port: 80
          resources:
            requests:
              cpu: 10m
              memory: 200Mi
            limits:
              memory: 500Mi
          instances: 2
          opsLogSidecar:
            resources:
              requests:
                cpu: 10m
                memory: 10Mi
              limits:
                memory: 100Mi
      storageClass:
        enabled: true
        name: ceph-object-nvme-2
        reclaimPolicy: Delete
        volumeBindingMode: Immediate
