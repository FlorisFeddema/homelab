route:
  enabled: true
  hostnames:
    - ceph.feddema.dev
  parentRefs:
    - name: envoy-gateway-public
      namespace: envoy-gateway
      sectionName: web-https

rook-ceph:
  revisionHistoryLimit: "0"
  resources:
    requests:
      cpu: 1
      memory: 128Mi
    limits:
      memory: 1Gi

  monitoring:
    enabled: true

  enableDiscoveryDaemon: true
  # discover:
  #   resources:
  #   - requests:
  #       cpu: 100m
  #       memory: 128Mi
  #   - limits:
  #       memory: 512Mi

  csi:
    enableMetadata: true
    enableOMAPGenerator: true
    enableLiveness: true
    serviceMonitor:
      enabled: true

    topology:
      enabled: true
      domainLabels:
        - kubernetes.io/hostname
        - topology.kubernetes.io/zone

    csiRBDProvisionerResource: |
      - name : csi-provisioner
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : csi-omap-generator
        resource:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiRBDPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 30Mi
          limits:
            memory: 100Mi
      - name : csi-rbdplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSProvisionerResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-provisioner
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSPluginResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 20Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi

#######################################################
rook-ceph-cluster:
  toolbox:
    enabled: true

  monitoring:
    enabled: true
    createPrometheusRules: true

  cephClusterSpec:
    cephVersion:
      image: quay.io/ceph/ceph:v19.2.2

    network:
      encryption:
        enabled: true
      compression:
        enabled: true
      requireMsgr2: true
      provider: host
      addressRanges:
        public: ['192.168.4.0/24']
        cluster: ['192.168.5.0/24']

    cephConfig:
      mgr:
        mgr/dashboard/standby_behaviour: 'error'
        mgr/dashboard/FEATURE_TOGGLE_NFS: 'false'
        mgr/dashboard/FEATURE_TOGGLE_ISCSI: 'false'
        mgr/dashboard/FEATURE_TOGGLE_MIRRORING: 'false'

    # cephConfigFromSecret:
    #   mgr:
    #     mgr/dashboard/GRAFANA_API_PASSWORD:
    #       name: grafana-api-credential # name of the Kubernetes secret
    #       key: password

    dashboard:
      ssl: false
#      prometheusEndpoint: http://prometheus-thanos-query.prometheus:9090/

    csi:
      readAffinity:
        enabled: true

    placement:
      mon:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: NotIn
                    values:
                      - kashaylan
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
      mgr:
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule

    mon:
      count: 3

    mgr:
      count: 2
      modules:
        - name: pg_autoscaler
          enabled: true
        - name: insights
          enabled: true
        - name: rook
          enabled: true
        - name: rgw
          enabled: true

    storage:
      useAllNodes: false
      useAllDevices: false
      allowDeviceClassUpdate: true
      nodes:
        - name: hortek-0
          devices:
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030303937365031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030323137395031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030343131325031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme

    resources:
      mgr:
        requests:
          cpu: 500m
          memory: 512Mi
        limits:
          memory: 1Gi
      mon:
        requests:
          cpu: 1000m
          memory: 1Gi
        limits:
          memory: 2Gi
      osd:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          memory: 4Gi
      prepareosd:
        requests:
          cpu: 500m
          memory: 50Mi
      mgr-sidecar:
        requests:
          cpu: 100m
          memory: 40Mi
        limits:
          memory: 100Mi
      crashcollector:
        requests:
          cpu: 10m
          memory: 50Mi
        limits:
          memory: 100Mi
      logcollector:
        requests:
          cpu: 10m
          memory: 10Mi
        limits:
          memory: 100Mi
      cleanup:
        requests:
          cpu: 500m
          memory: 100Mi
        limits:
          memory: 1Gi
      exporter:
        requests:
          cpu: 50m
          memory: 50Mi
        limits:
          memory: 128Mi


  cephBlockPools:
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
    - name: ceph-blockpool-nvme
      spec:
        failureDomain: zone
        replicated:
          size: 2
        enableRBDStats: true
        deviceClass: nvme
      storageClass:
        enabled: true
        name: ceph-block-nvme
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    - name: ceph-blockpool-hdd
      spec:
        failureDomain: zone
        replicated:
          size: 1
        enableRBDStats: true
        deviceClass: hdd
      storageClass:
        enabled: true
        name: ceph-block-hdd
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

  cephFileSystems: []

  cephObjectStores:
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: zone
          replicated:
            size: 2
        dataPool:
          failureDomain: zone
          replicated:
            size: 2
        allowUsersInNamespaces: ['*']
        gateway:
          port: 80
          resources:
            requests:
              cpu: 10m
              memory: 200Mi
            limits:
              memory: 500Mi
          instances: 2
          opsLogSidecar:
            resources:
              requests:
                cpu: 10m
                memory: 10Mi
              limits:
                memory: 100Mi
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: Immediate
