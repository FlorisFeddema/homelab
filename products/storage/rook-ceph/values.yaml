gatus-monitor:
  appName: ceph
  publicEndpoints:
    - name: ceph
  privateEndpoints:
    - name: ceph

rook-ceph:
  revisionHistoryLimit: "0"
  resources:
    requests:
      cpu: 1
      memory: 128Mi
    limits:
      memory: 1Gi

  monitoring:
    enabled: true
  obcAllowAdditionalConfigFields: "maxObjects,maxSize,bucketOwner"

  csi:
    rookUseCsiOperator: true
    enableMetadata: true
    enableOMAPGenerator: true
    enableLiveness: true
    serviceMonitor:
      enabled: true
    cephFSKernelMountOptions: "ms_mode=secure"
    provisionerReplicas: 1
    csiAddons:
      enabled: true

    topology:
      enabled: true
      domainLabels:
        - kubernetes.io/hostname
        - topology.kubernetes.io/zone

    csiRBDProvisionerResource: |
      - name : csi-provisioner
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : csi-rbdplugin
        resource:
          requests:
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : csi-omap-generator
        resource:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            memory: 1Gi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            memory: 256Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiRBDPluginResource: |
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 30Mi
          limits:
            memory: 100Mi
      - name : csi-rbdplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSProvisionerResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-provisioner
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-resizer
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-attacher
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : csi-snapshotter
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi
    csiCephFSPluginResource: |
      - name : csi-cephfsplugin
        resource:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            memory: 200Mi
      - name : driver-registrar
        resource:
          requests:
            cpu: 10m
            memory: 20Mi
          limits:
            memory: 100Mi
      - name : liveness-prometheus
        resource:
          requests:
            cpu: 10m
            memory: 50Mi
          limits:
            memory: 100Mi
      - name : log-collector
        resource:
          requests:
            cpu: 10m
            memory: 15Mi
          limits:
            memory: 100Mi

#######################################################
rook-ceph-cluster:
  toolbox:
    enabled: true

  route:
    dashboard:
      host:
        name: ceph.feddema.dev
        path: "/"
        pathType: PathPrefix
      parentRefs:
        - group: gateway.networking.k8s.io
          kind: Gateway
          name: envoy-gateway-public
          namespace: envoy-gateway
          sectionName: web-https

  monitoring:
    enabled: true
    createPrometheusRules: true

  cephClusterSpec:
    cephVersion:
      image: quay.io/ceph/ceph:v19.2.3

    network:
      connections:
        encryption:
          enabled: true
        compression:
          enabled: true
        requireMsgr2: true
      provider: host
      addressRanges:
        public: ['192.168.4.0/24']
        cluster: ['192.168.5.0/24']

    cephConfig:
      global:
        osd_pool_default_size: '3'
        osd_pool_default_min_size: '2'
        mon_data_avail_warn: '15'
      osd:
        osd_mclock_profile: "high_recovery_ops"
      mgr:
        mgr/dashboard/standby_behaviour: 'error'
        mgr/dashboard/FEATURE_TOGGLE_NFS: 'false'
        mgr/dashboard/FEATURE_TOGGLE_ISCSI: 'false'
        mgr/dashboard/FEATURE_TOGGLE_MIRRORING: 'false'

    dashboard:
      ssl: false
      prometheusEndpoint: http://prometheus-thanos-query.prometheus:9090/

    csi:
      readAffinity:
        enabled: true

    placement:
      mon:
        tolerations:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: NotIn
                    values:
                      - "korris-1"
      mgr:
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule

    mon:
      count: 3

    mgr:
      count: 2
      modules:
        - name: pg_autoscaler
          enabled: true
        - name: insights
          enabled: true
        - name: rgw
          enabled: true
        - name: rook
          enabled: true

    storage:
      useAllNodes: false
      useAllDevices: false
      allowDeviceClassUpdate: true
      config:
        enableCrushUpdates: 'true'
      nodes:
        - name: hortek-0
          devices:
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030303937365031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-eui.e8238fa6bf530001001b448b4aee9594
              deviceClass: nvme
            - name: /dev/disk/by-id/nvme-eui.0025385a91b29401
              deviceClass: nvme
            - name: /dev/disk/by-id/ata-OOS20000G_0004WR10
              deviceClass: hdd
#        - name: korris-1
#          devices:
#            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030323137395031313130-4c6578617220535344204e4d36323020325442-00000001
#              deviceClass: nvme
        - name: tasuq-1
          devices:
            - name: /dev/disk/by-id/nvme-nvme.1e4b-4e4448353735523030343131325031313130-4c6578617220535344204e4d36323020325442-00000001
              deviceClass: nvme
        - name: kashaylan-3
          devices:
            - name: /dev/disk/by-id/nvme-eui.e8238fa6bf530001001b448b4aeeb60d
              deviceClass: nvme

    resources:
      mgr:
        requests:
          cpu: 500m
          memory: 512Mi
        limits:
          memory: 2Gi
      mon:
        requests:
          cpu: 1000m
          memory: 1300Mi
        limits:
          memory: 2500Mi
      osd:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          memory: 4Gi
      prepareosd:
        requests:
          cpu: 500m
          memory: 50Mi
      mgr-sidecar:
        requests:
          cpu: 100m
          memory: 40Mi
        limits:
          memory: 100Mi
      crashcollector:
        requests:
          cpu: 10m
          memory: 50Mi
        limits:
          memory: 100Mi
      logcollector:
        requests:
          cpu: 10m
          memory: 10Mi
        limits:
          memory: 100Mi
      cleanup:
        requests:
          cpu: 500m
          memory: 100Mi
        limits:
          memory: 1Gi
      exporter:
        requests:
          cpu: 50m
          memory: 50Mi
        limits:
          memory: 128Mi

  cephBlockPoolsVolumeSnapshotClass:
    enabled: true
    name: ceph-block
    isDefault: true
    deletionPolicy: Delete
    labels:
      velero.io/csi-volumesnapshot-class: "true"

  cephFileSystemVolumeSnapshotClass:
    enabled: true
    name: ceph-filesystem
    isDefault: false
    deletionPolicy: Delete
    labels:
      velero.io/csi-volumesnapshot-class: "true"

  cephBlockPools:
    - name: ceph-block-nvme-1
      spec:
        failureDomain: zone
        replicated:
          size: 2
        parameters:
          min_size: '1'
        enableRBDStats: true
        # deviceClass: nvme
        enableCrushUpdates: true
      storageClass:
        enabled: true
        name: ceph-block-nvme-1
        annotations:
          reclaimspace.csiaddons.openshift.io/schedule: "@midnight"
        isDefault: false
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    - name: ceph-block-nvme-2
      spec:
        failureDomain: zone
        replicated:
          size: 2
        parameters:
          min_size: '1'
        enableRBDStats: true
        # deviceClass: nvme
        enableCrushUpdates: true
      storageClass:
        enabled: true
        name: ceph-block-nvme-2
        annotations:
          reclaimspace.csiaddons.openshift.io/schedule: "@midnight"
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

  cephFileSystems:
    - name: ceph-file-nvme-2
      spec:
        metadataPool:
          enableRBDStats: true
          # deviceClass: nvme
          enableCrushUpdates: true
          failureDomain: zone  
          replicated:
            size: 2
          parameters:
            min_size: '1'

        dataPools:
          - name: data0
            enableRBDStats: true
            # deviceClass: nvme
            enableCrushUpdates: true
            failureDomain: zone
            replicated:
              size: 2
            parameters:
              min_size: '1'
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            requests:
              cpu: 10m
              memory: 1Gi
            limits:
              memory: 3Gi
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-file-nvme-2
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: { }
        labels: { }
        mountOptions: [ ]
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    - name: ceph-file-hdd
      spec:
        metadataPool:
          enableRBDStats: true
          # deviceClass: hdd
          failureDomain: osd
          enableCrushUpdates: true
          parameters:
            min_size: '1'
          replicated:
            size: 2
        dataPools:
          - name: data0
            enableRBDStats: true
            # deviceClass: hdd
            enableCrushUpdates: true
            failureDomain: osd
            replicated:
              size: 2
            parameters:
              min_size: '1'
        metadataServer:
          activeCount: 1
          activeStandby: false
          placement:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - hortek-0
          resources:
            requests:
              cpu: 10m
              memory: 512Mi
            limits:
              memory: 3Gi
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-file-hdd
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: { }
        labels: { }
        mountOptions: [ ]
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

  cephObjectStores:
    - name: ceph-object-nvme-2
      spec:
        metadataPool:
          failureDomain: zone
          enableRBDStats: true
          # deviceClass: nvme
          enableCrushUpdates: true
          replicated:
            size: 2
          parameters:
            min_size: '1'
        dataPool:
          failureDomain: zone
          enableRBDStats: true
          # deviceClass: nvme
          enableCrushUpdates: true
          replicated:
            size: 2
          parameters:
            min_size: '1'
        allowUsersInNamespaces: ['*']
        gateway:
          port: 80
          resources:
            requests:
              cpu: 10m
              memory: 200Mi
            limits:
              memory: 500Mi
          instances: 2
          opsLogSidecar:
            resources:
              requests:
                cpu: 10m
                memory: 10Mi
              limits:
                memory: 100Mi
      storageClass:
        enabled: true
        name: ceph-object-nvme-2
        reclaimPolicy: Delete
        volumeBindingMode: Immediate
